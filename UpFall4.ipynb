{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYjxy8RQC_g5"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "imXIpcuRC_hC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "from torch import matmul\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QJ3Hlm7tC_hG"
      },
      "outputs": [],
      "source": [
        "dataframe1 = pd.read_csv('CompleteDataSet.csv', dtype=\"str\")\n",
        "\n",
        "dataset = dataframe1.values.tolist()\n",
        "\n",
        "new_dataset = np.array(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "aB36QIyRC_hH"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in range(1,4):\n",
        "    # Trial\n",
        "    for j in range(1,12):\n",
        "        # Activity\n",
        "        for k in range(1,18):\n",
        "            # Subject\n",
        "            z = []\n",
        "            # Each Video\n",
        "            for n in range(1,len(new_dataset)-1):\n",
        "                if (float(new_dataset[n][45]) == i) & (float(new_dataset[n][44]) == j) & (float(new_dataset[n][43]) == k):\n",
        "                    a = new_dataset[n,15:18]\n",
        "                    a = a.astype(np.float32)\n",
        "                    b = new_dataset[n,29:32]\n",
        "                    b = b.astype(np.float32)\n",
        "                    c = np.concatenate((a,b), axis = 0)\n",
        "                    z.append(c)\n",
        "            X.append(z)\n",
        "            y.append(j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V23L2omDC_hJ",
        "outputId": "de15209d-0283-4626-df52-7d3c4dc95565"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.pop(551)\n",
        "X.pop(364)\n",
        "y.pop(551)\n",
        "y.pop(364)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuFrceS_C_hK",
        "outputId": "a3fee069-a397-4a29-e882-b4db2b1f092e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avg: 527.1502683363149\n",
            "Min: 140\n",
            "Max: 1145\n"
          ]
        }
      ],
      "source": [
        "counter = 0\n",
        "max = 0\n",
        "min = 1000\n",
        "for i in range(len(X)):\n",
        "    counter += len(X[i])\n",
        "    if len(X[i]) == 0:\n",
        "        print(i)\n",
        "    if len(X[i]) > max:\n",
        "        max = len(X[i])\n",
        "    if len(X[i]) < min:\n",
        "        min = len(X[i])\n",
        "\n",
        "print(f\"Avg: {counter/len(X)}\\nMin: {min}\\nMax: {max}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "YCEPfmjrC_hL"
      },
      "outputs": [],
      "source": [
        "def to_np_array(X):\n",
        "    for i in range(len(X)):\n",
        "        X[i] = np.array(X[i])\n",
        "        for j in range(len(X[i])):\n",
        "            X[i][j] = np.array(X[i][j])\n",
        "    X = np.array(X)\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jn5Ha4FQC_hM",
        "outputId": "656ddd26-a273-4fd8-de47-325cd3ac772b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18824\\3925073110.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.array(X)\n"
          ]
        }
      ],
      "source": [
        "X_train = []\n",
        "X_test = []\n",
        "y_train = []\n",
        "y_test = []\n",
        "\n",
        "for i in range(1,12):\n",
        "    counter = 0\n",
        "    for j in range(len(y)):\n",
        "        if (i == y[j]) & (counter < 7):\n",
        "            X_test.append(X[j])\n",
        "            y_test.append(y[j])\n",
        "            counter += 1\n",
        "        elif (i == y[j]) & (counter >= 7):\n",
        "            X_train.append(X[j])\n",
        "            y_train.append(y[j])\n",
        "            counter += 1\n",
        "\n",
        "X_train = to_np_array(X_train)\n",
        "X_test = to_np_array(X_test)\n",
        "y_train = np.array(y_train, dtype=np.uint8)\n",
        "y_test = np.array(y_test, dtype=np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVGhq2TcnfPF",
        "outputId": "5bc5555d-94ca-472a-b4e6-ee52f3ed32bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "482\n",
            "77\n"
          ]
        }
      ],
      "source": [
        "print(len(X_train))\n",
        "print(len(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "XZvJw2v5JYwf"
      },
      "outputs": [],
      "source": [
        "# 1:Fall    0:NotFall\n",
        "def to_binary(y):\n",
        "  for i in range(len(y)):\n",
        "    if (y[i] >= 6) & (y[i] <= 11):\n",
        "      y[i] = 0\n",
        "    elif (y[i] >= 1) & (y[i] <= 5):\n",
        "      y[i] = 1\n",
        "    else:\n",
        "      print(i)\n",
        "  return y\n",
        "\n",
        "y_train = to_binary(y_train)\n",
        "y_test = to_binary(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnXMg10EC_hP",
        "outputId": "a8d4e40b-b059-4c36-fbb7-3ae28db48dbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.float32'>\n"
          ]
        }
      ],
      "source": [
        "print(type(X_train))\n",
        "print(type(X_train[0]))\n",
        "print(type(X_train[0][0]))\n",
        "print(type(X_train[0][0][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbzJUlW9C_hQ"
      },
      "source": [
        "# PyTorch"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQyqgwTTC_hR",
        "outputId": "4e6cc5b7-8d2a-42fe-a914-38a988d53d65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([482, 1145, 6])\n",
            "torch.Size([77, 1145, 6])\n"
          ]
        }
      ],
      "source": [
        "X_train_tensor = torch.zeros((len(X_train),max, 6))\n",
        "X_test_tensor = torch.zeros((len(X_test),max, 6))\n",
        "y_train_tensor = torch.Tensor(y_train)\n",
        "y_test_tensor = torch.Tensor(y_test)\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "    for j in range(len(X_train[i])):\n",
        "        for k in range(len(X_train[i][j])):\n",
        "            X_train_tensor[i][j][k] = float(X_train[i][j][k])\n",
        "for i in range(len(X_test)):\n",
        "    for j in range(len(X_test[i])):\n",
        "        for k in range(len(X_test[i][j])):\n",
        "            X_test_tensor[i][j][k] = float(X_test[i][j][k])\n",
        "\n",
        "print(X_train_tensor.shape)\n",
        "print(X_test_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ns7stPyoC_hU"
      },
      "outputs": [],
      "source": [
        "train_dataset = []\n",
        "for i in range(len(X_train_tensor)):\n",
        "    a = (X_train_tensor[i], y_train_tensor[i])\n",
        "    train_dataset.append(a)\n",
        "test_dataset = []\n",
        "for i in range(len(X_test_tensor)):\n",
        "    a = (X_test_tensor[i], y_test_tensor[i])\n",
        "    test_dataset.append(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_YxNHhUig4ps"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "batch_size = 19\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "3nI5rlLkC_hV"
      },
      "outputs": [],
      "source": [
        "# Fully connected neural network with one hidden layer\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # -> x needs to be: (batch_size, seq, input_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.dropout = nn.Dropout(p=0.4)\n",
        "\n",
        "        # or:\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_size, num_classes)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Set initial hidden states (and cell states for LSTM)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "        \n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "        \n",
        "        # Forward propagate RNN\n",
        "        out, _ = self.gru(x, h0)  \n",
        "        # or:\n",
        "        # out, _ = self.lstm(x, (h0,c0))  \n",
        "        \n",
        "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        # out: (n, 28, 128)\n",
        "        \n",
        "        # Decode the hidden state of the last time step\n",
        "        out = out[:, -1, :]\n",
        "        # out: (n, 128)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu(out)\n",
        "        # out = self.fc2(out)\n",
        "        # out = self.relu(out)\n",
        "        # out = self.fc2(out)\n",
        "        # out = self.relu(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.softmax(out)\n",
        "        # out: (n, 10)\n",
        "        return out\n",
        "\n",
        "# num_classes = 2\n",
        "# input_size = 6\n",
        "# hidden_size = 12\n",
        "# num_layers = 2\n",
        "# rnn = RNN(input_size, hidden_size, num_layers, num_classes)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZUPXJjFtyOJ"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  \n",
        "  def __init__(self, emb_dim, model_dim):\n",
        "    super(SelfAttention, self).__init__()\n",
        "    self.emb_dim = emb_dim\n",
        "    self.model_dim = model_dim\n",
        "    \n",
        "    self.to_query = nn.Linear(emb_dim, model_dim, bias=True)\n",
        "    # self.to_query.weight = nn.Parameter(w_query.t())\n",
        "    \n",
        "    self.to_key = nn.Linear(emb_dim, model_dim, bias=True)\n",
        "    # self.to_key.weight = nn.Parameter(w_key.t())\n",
        "    \n",
        "    self.to_value = nn.Linear(emb_dim, model_dim, bias=True)\n",
        "    # self.to_value.weight = nn.Parameter(w_value.t())\n",
        "\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    q = self.to_query(inputs)\n",
        "    k = self.to_key(inputs)\n",
        "    v = self.to_value(inputs)\n",
        "\n",
        "    attn_score = matmul(q, k.transpose(-1, -2))\n",
        "    softmax_attn_score = softmax(attn_score, dim=-1)\n",
        "    output = matmul(softmax_attn_score, v)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5vA28tMgeJ1",
        "outputId": "b325dc10-a036-42c7-f5dc-28b213aaae63"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, heads=2, p=0.4, num_classes=2):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.heads = heads\n",
        "    self.d_model = d_model\n",
        "    heads_dim = int(d_model / heads)\n",
        "    self.heads_dim = heads_dim\n",
        "    self.p = p\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "    self.to_query = nn.Linear(d_model, heads * heads_dim, bias=True)\n",
        "    self.to_key = nn.Linear(d_model, heads * heads_dim, bias=True)\n",
        "    self.to_value = nn.Linear(d_model, heads * heads_dim, bias=True)\n",
        "\n",
        "    self.unify_heads = nn.Linear(heads * heads_dim, d_model, bias=True)\n",
        "    self.fc_final = nn.Linear(d_model, 2)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=p)\n",
        "    self.norm = nn.LayerNorm(self.d_model)\n",
        "  \n",
        "  def forward(self, inputs, mask=None, kv=None):\n",
        "    batch_size, sequence_length, emb_dim = inputs.shape\n",
        "    \n",
        "    if kv is not None:\n",
        "      kv = kv\n",
        "    else:\n",
        "      kv = inputs\n",
        "    kv_batch_szie, kv_sequence_lentgh, _ = kv.size()\n",
        "\n",
        "    # Head\n",
        "    inputs = self.norm(inputs)\n",
        "\n",
        "    q = self.to_query(inputs).view(batch_size, sequence_length, self.heads, self.heads_dim).transpose(1, 2)\n",
        "    k = self.to_key(kv).view(kv_batch_szie, kv_sequence_lentgh, self.heads, self.heads_dim).transpose(1, 2)\n",
        "    v = self.to_value(kv).view(kv_batch_szie, kv_sequence_lentgh, self.heads, self.heads_dim).transpose(1, 2)\n",
        "\n",
        "    attn_score = matmul(q, k.transpose(-1, -2))\n",
        "    \n",
        "    # Scale After Dot-Product\n",
        "    attn_score = attn_score / self.heads_dim ** (1/float(2))\n",
        "    \n",
        "    # Mask\n",
        "    if mask is not None:\n",
        "      attn_score = attn_score.masked_fill(mask==1, value=-1e9)\n",
        "    \n",
        "    # Softmax\n",
        "    softmax_attn_score = softmax(attn_score, dim=-1)\n",
        "    \n",
        "    output = matmul(softmax_attn_score, v)\n",
        "    output = output.transpose(1, 2).contiguous().view(batch_size, sequence_length, self.heads * self.heads_dim)\n",
        "    output_final = self.unify_heads(output)\n",
        "    \n",
        "    # output_final = self.relu(output_final)\n",
        "    # output_final = self.dropout(output_final)\n",
        "    # predictions = self.fc_final(output_final)\n",
        "    # predictions = self.softmax(predictions)\n",
        "    \n",
        "    num_classes = 2\n",
        "    input_size = 6\n",
        "    hidden_size = 12\n",
        "    num_layers = 2\n",
        "    rnn = RNN(input_size, hidden_size, num_layers, num_classes)\n",
        "    output_final = rnn(output_final)\n",
        "    \n",
        "    return output_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, heads=2, p=0.4, num_classes=2):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    # self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "    # -> x needs to be: (batch_size, seq, input_size)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "    self.dropout = nn.Dropout(p=0.4)\n",
        "\n",
        "    self.input_size = 6\n",
        "    self.hidden_size = 12\n",
        "    self.num_layers = 2\n",
        "    self.num_classes = num_classes\n",
        "    # or:\n",
        "    # self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "    self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True)\n",
        "    self.fc1 = nn.Linear(self.hidden_size, self.num_classes)\n",
        "    self.fc2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "\n",
        "    self.heads = heads\n",
        "    self.d_model = d_model\n",
        "    heads_dim = int(d_model / heads)\n",
        "    self.heads_dim = heads_dim\n",
        "    self.p = p\n",
        "\n",
        "    self.to_query = nn.Linear(d_model, heads * heads_dim, bias=True)\n",
        "    self.to_key = nn.Linear(d_model, heads * heads_dim, bias=True)\n",
        "    self.to_value = nn.Linear(d_model, heads * heads_dim, bias=True)\n",
        "\n",
        "    self.unify_heads = nn.Linear(heads * heads_dim, d_model, bias=True)\n",
        "    self.fc_final = nn.Linear(d_model, 2)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=p)\n",
        "    self.norm = nn.LayerNorm(self.d_model)\n",
        "  \n",
        "  def forward(self, inputs, mask=None, kv=None):\n",
        "    batch_size, sequence_length, emb_dim = inputs.shape\n",
        "    \n",
        "    if kv is not None:\n",
        "      kv = kv\n",
        "    else:\n",
        "      kv = inputs\n",
        "    kv_batch_szie, kv_sequence_lentgh, _ = kv.size()\n",
        "\n",
        "    # Head\n",
        "    inputs = self.norm(inputs)\n",
        "\n",
        "    q = self.to_query(inputs).view(batch_size, sequence_length, self.heads, self.heads_dim).transpose(1, 2)\n",
        "    k = self.to_key(kv).view(kv_batch_szie, kv_sequence_lentgh, self.heads, self.heads_dim).transpose(1, 2)\n",
        "    v = self.to_value(kv).view(kv_batch_szie, kv_sequence_lentgh, self.heads, self.heads_dim).transpose(1, 2)\n",
        "\n",
        "    attn_score = matmul(q, k.transpose(-1, -2))\n",
        "    \n",
        "    # Scale After Dot-Product\n",
        "    attn_score = attn_score / self.heads_dim ** (1/float(2))\n",
        "    \n",
        "    # Mask\n",
        "    if mask is not None:\n",
        "      attn_score = attn_score.masked_fill(mask==1, value=-1e9)\n",
        "    \n",
        "    # Softmax\n",
        "    softmax_attn_score = softmax(attn_score, dim=-1)\n",
        "    \n",
        "    output = matmul(softmax_attn_score, v)\n",
        "    output = output.transpose(1, 2).contiguous().view(batch_size, sequence_length, self.heads * self.heads_dim)\n",
        "    output_final = self.unify_heads(output)\n",
        "    \n",
        "    # output_final = self.relu(output_final)\n",
        "    # output_final = self.dropout(output_final)\n",
        "    # predictions = self.fc_final(output_final)\n",
        "    # predictions = self.softmax(predictions)\n",
        "    \n",
        "    x = output_final\n",
        "    # Set initial hidden states (and cell states for LSTM)\n",
        "    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "    c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "    \n",
        "    # x: (n, 28, 28), h0: (2, n, 128)\n",
        "    \n",
        "    # Forward propagate RNN\n",
        "    # out, _ = self.gru(x, h0)  \n",
        "    # or:\n",
        "    out, _ = self.lstm(x, (h0,c0))  \n",
        "    \n",
        "    # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "    # out: (n, 28, 128)\n",
        "    \n",
        "    # Decode the hidden state of the last time step\n",
        "    out = out[:, -1, :]\n",
        "    # out: (n, 128)\n",
        "    out = self.dropout(out)\n",
        "    out = self.fc2(out)\n",
        "    out = self.relu(out)\n",
        "    # out = self.fc2(out)\n",
        "    # out = self.relu(out)\n",
        "    # out = self.fc2(out)\n",
        "    # out = self.relu(out)\n",
        "    out = self.fc1(out)\n",
        "    out = self.softmax(out)\n",
        "    \n",
        "    return output_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "File GRU.pt cannot be opened.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49msave(model, \u001b[39m\"\u001b[39;49m\u001b[39mGRU.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\Sepehr\\lib\\site-packages\\torch\\serialization.py:422\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    419\u001b[0m _check_dill_version(pickle_module)\n\u001b[0;32m    421\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 422\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    423\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    424\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\Sepehr\\lib\\site-packages\\torch\\serialization.py:309\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 309\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\Sepehr\\lib\\site-packages\\torch\\serialization.py:287\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 287\u001b[0m     \u001b[39msuper\u001b[39m(_open_zipfile_writer_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileWriter(\u001b[39mstr\u001b[39;49m(name)))\n",
            "\u001b[1;31mRuntimeError\u001b[0m: File GRU.pt cannot be opened."
          ]
        }
      ],
      "source": [
        "torch.save(model, \"GRU.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1145])\n"
          ]
        }
      ],
      "source": [
        "model = MultiHeadAttention(d_model=6, heads=1)\n",
        "# test_sample = X_test_tensor[:2]\n",
        "test_sample = torch.randn((2, 1145, 6))\n",
        "\n",
        "pad = 0\n",
        "# trg = torch.tensor([[1, 2, 0, 0, 0], [1, 2, 3, 0, 0]])\n",
        "bs, seq_len, emb_dim = test_sample.shape\n",
        "mask_shape = (1, seq_len, seq_len, emb_dim)\n",
        "ones_tensor = torch.ones(mask_shape)\n",
        "triu_tesnor = torch.triu(ones_tensor, diagonal=1).type(torch.int16)\n",
        "trg_mask = (test_sample == pad).type(torch.int16).unsqueeze(-2)\n",
        "mask = triu_tesnor | trg_mask\n",
        "mask = mask.unsqueeze(1)\n",
        "\n",
        "# output = model(test_sample, mask)\n",
        "output = model(test_sample)\n",
        "print(output[:,:,0].shape)\n",
        "# print(y_test[:2].long())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.3748, 0.6252],\n",
            "        [0.3808, 0.6192],\n",
            "        [0.3658, 0.6342],\n",
            "        [0.3795, 0.6205]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([1, 1, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "multi_head_attention = MultiHeadAttention(d_model=6, heads=1)\n",
        "\n",
        "sample_dataset = test_dataset[:4]\n",
        "\n",
        "sample_loader = torch.utils.data.DataLoader(dataset=sample_dataset, \n",
        "                                            batch_size=4, \n",
        "                                            shuffle=False)\n",
        "\n",
        "for sample, label in sample_loader:\n",
        "    # print(sample.shape)\n",
        "    # print(label.shape)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    outputs = multi_head_attention(sample)\n",
        "    outputs = rnn(outputs)\n",
        "    print(outputs)\n",
        "    # print(outputs[:,:,0].shape)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    print(predicted)\n",
        "    # print(predicted[:,1])\n",
        "    # print(label.long())\n",
        "    # print(label.long().shape)\n",
        "    # loss = criterion(outputs[:,:,0], label.long())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, d_model, heads, ff_size=2096, dropout=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.attention = MultiHeadAttention(d_model, heads)\n",
        "    self.attn_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    self.ff = nn.Sequential(\n",
        "      nn.Linear(d_model, ff_size),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(dropout),\n",
        "      nn.Linear(ff_size, d_model)\n",
        "    )\n",
        "    \n",
        "    self.final_norm = nn.LayerNorm(d_model)\n",
        "    self.do = nn.Dropout(dropout)\n",
        "  \n",
        "\n",
        "  def forward(self, src_embeddings, src_mask=None):\n",
        "    tensor = tensor + self.do(self.attention(self.attn_norm(src_embeddings), src_mask))\n",
        "\n",
        "    tensor = tensor + self.do(self.ff(self.final_norm(tensor)))\n",
        "\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, d_model, heads, num_layers=2, ff_size=2096, dropout=0.1):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "\n",
        "    self.layers = nn.ModuleList()\n",
        "    for i in range(num_layers):\n",
        "      self.layers.append(Encoder(d_model, heads, ff_size, dropout))\n",
        "    self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "  \n",
        "  def forward(self, src_embeddings, src_mask=None):\n",
        "    hidden_states = []\n",
        "    \n",
        "    for layer in self.layers:\n",
        "      src_embeddings = layer(src_embeddings, src_mask)\n",
        "      hidden_states.append(src_embeddings)\n",
        "    \n",
        "    return hidden_states, self.norm(hidden_states[-1])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, d_model, heads, ff_size=2096, dropout=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.attention = MultiHeadAttention(d_model, heads)\n",
        "    self.attn_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    self.encoder_decoder_attn = MultiHeadAttention(d_model, heads)\n",
        "    self.encoder_decoder_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    self.ff = nn.Sequential(\n",
        "      nn.Linear(d_model, ff_size),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(dropout),\n",
        "      nn.Linear(ff_size, d_model)\n",
        "    )\n",
        "    \n",
        "    self.final_norm = nn.LayerNorm(d_model)\n",
        "    self.do = nn.Dropout(dropout)\n",
        "  \n",
        "\n",
        "  def forward(self, src_embeddings, memory, src_mask=None, trg_mask=None):\n",
        "    tensor = tensor + self.do(self.attention(self.attn_norm(src_embeddings), src_mask))\n",
        "    tensor = tensor + self.do(self.encoder_decoder_attn(self.encoder_decoder_norm(src_embeddings), kv=memory, mask=src_mask))\n",
        " \n",
        "    tensor = tensor + self.do(self.ff(self.final_norm(tensor)))\n",
        "\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "  def __init__(self, d_model, heads, num_layers=2, ff_size=2096, dropout=0.1):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "\n",
        "    self.layers = nn.ModuleList()\n",
        "    for i in range(num_layers):\n",
        "      self.layers.append(Decoder(d_model, heads, ff_size, dropout))\n",
        "    self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "  \n",
        "  def forward(self, src_embeddings, memory, src_mask=None, trg_mask=None):\n",
        "    hidden_states = []\n",
        "    \n",
        "    for layer in self.layers:\n",
        "      src_embeddings = layer(src_embeddings, memory, src_mask, trg_mask)\n",
        "      hidden_states.append(src_embeddings)\n",
        "    \n",
        "    return hidden_states, self.norm(hidden_states[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "only one element tensors can be converted to Python scalars",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m sequence_length, d_model \u001b[39m=\u001b[39m test_sample\n\u001b[1;32m----> 2\u001b[0m model_encoder \u001b[39m=\u001b[39m TransformerEncoder(d_model\u001b[39m=\u001b[39;49md_model, heads\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, num_layers\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m hidden_encoder, output_encoder \u001b[39m=\u001b[39m model_encoder(test_sample)\n\u001b[0;32m      5\u001b[0m model_decoder \u001b[39m=\u001b[39m TransformerDecoder(d_model\u001b[39m=\u001b[39md_model, heads\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, num_layers\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
            "Cell \u001b[1;32mIn[23], line 7\u001b[0m, in \u001b[0;36mTransformerEncoder.__init__\u001b[1;34m(self, d_model, heads, num_layers, ff_size, dropout)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList()\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_layers):\n\u001b[1;32m----> 7\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mappend(Encoder(d_model, heads, ff_size, dropout))\n\u001b[0;32m      8\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(d_model)\n",
            "Cell \u001b[1;32mIn[22], line 5\u001b[0m, in \u001b[0;36mEncoder.__init__\u001b[1;34m(self, d_model, heads, ff_size, dropout)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, d_model, heads, ff_size\u001b[39m=\u001b[39m\u001b[39m2096\u001b[39m, dropout\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m):\n\u001b[0;32m      3\u001b[0m   \u001b[39msuper\u001b[39m(Encoder, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m----> 5\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention \u001b[39m=\u001b[39m MultiHeadAttention(d_model, heads)\n\u001b[0;32m      6\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_norm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(d_model)\n\u001b[0;32m      8\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m      9\u001b[0m     nn\u001b[39m.\u001b[39mLinear(d_model, ff_size),\n\u001b[0;32m     10\u001b[0m     nn\u001b[39m.\u001b[39mReLU(),\n\u001b[0;32m     11\u001b[0m     nn\u001b[39m.\u001b[39mDropout(dropout),\n\u001b[0;32m     12\u001b[0m     nn\u001b[39m.\u001b[39mLinear(ff_size, d_model)\n\u001b[0;32m     13\u001b[0m   )\n",
            "Cell \u001b[1;32mIn[17], line 6\u001b[0m, in \u001b[0;36mMultiHeadAttention.__init__\u001b[1;34m(self, d_model, heads)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads \u001b[39m=\u001b[39m heads\n\u001b[0;32m      5\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model \u001b[39m=\u001b[39m d_model\n\u001b[1;32m----> 6\u001b[0m heads_dim \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(d_model \u001b[39m/\u001b[39;49m heads)\n\u001b[0;32m      7\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads_dim \u001b[39m=\u001b[39m heads_dim\n\u001b[0;32m      8\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_query \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(d_model, heads \u001b[39m*\u001b[39m heads_dim, bias\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
          ]
        }
      ],
      "source": [
        "sequence_length, d_model = test_sample\n",
        "model_encoder = TransformerEncoder(d_model=d_model, heads=1, num_layers=3)\n",
        "hidden_encoder, output_encoder = model_encoder(test_sample)\n",
        "\n",
        "model_decoder = TransformerDecoder(d_model=d_model, heads=1, num_layers=3)\n",
        "hidden_decoder, output_decoder = model_decoder(test_sample, output_encoder)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Refrence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiheadedAttention(nn.Module):\n",
        "    def __init__(self, d_model, heads=2):\n",
        "        super(MultiheadedAttention, self).__init__()\n",
        "\n",
        "        self.heads = heads\n",
        "\n",
        "        # The model dimension split into n-heads\n",
        "        self.heads_dim = int(d_model / heads)\n",
        "\n",
        "        # In final implementation, we must use bias=True\n",
        "        self.to_query = nn.Linear(d_model, heads * self.heads_dim, bias=False)\n",
        "        # self.to_query.weight = nn.Parameter(w_query.t())  # This should be commented in final implementation\n",
        "\n",
        "        # In final implementation, we must use bias=True\n",
        "        self.to_key = nn.Linear(d_model, heads * self.heads_dim, bias=False)\n",
        "        # self.to_key.weight = nn.Parameter(w_key.t())  # This should be commented in final implementation\n",
        "\n",
        "        # In final implementation, we must use bias=True\n",
        "        self.to_value = nn.Linear(d_model, heads * self.heads_dim, bias=False)\n",
        "        # self.to_value.weight = nn.Parameter(w_value.t())  # This should be commented in final implementation\n",
        "\n",
        "        # In final implementation, we must use bias=True\n",
        "        self.unify_heads = nn.Linear(heads * self.heads_dim, d_model, bias=False)\n",
        "        # self.unify_heads.weight = nn.Parameter(w_unify_heads.t())  # This should be commented in final implementation\n",
        "\n",
        "    def forward(self, inputs, mask=None, kv=None):\n",
        "        # Create Q, K, and V using input vectors\n",
        "        bs, seq, emb_dim = inputs.shape\n",
        "\n",
        "        if kv is not None:\n",
        "            kv = kv\n",
        "        else:\n",
        "            kv = inputs\n",
        "\n",
        "        kv_bs, kv_seq_len, _ = kv.size()\n",
        "\n",
        "        # Transpose: bs x seq-length x num-heads x heads_dim -> bs x num-heads x seq-length x heads_dim\n",
        "        q = self.to_query(inputs).view(bs, seq, self.heads, self.heads_dim).transpose(1, 2)\n",
        "        k = self.to_key(kv).view(kv_bs, kv_seq_len, self.heads, self.heads_dim).transpose(1, 2)\n",
        "        v = self.to_value(kv).view(kv_bs, kv_seq_len, self.heads, self.heads_dim).transpose(1, 2)\n",
        "\n",
        "        # Scale before Dot-product: q/root_forth(head_dim) and k/root_forth(head_dim)\n",
        "        q = q / (self.heads_dim ** 1 / float(4))\n",
        "        k = k / (self.heads_dim ** 1 / float(4))\n",
        "\n",
        "        # Compute Attention scores\n",
        "        attn_scores = matmul(q, k.transpose(-1, -2))\n",
        "        # Scale after Dot-product : attn_scores/root_square(head_dim)\n",
        "        # attn_scores = attn_scores/(self.heads_dim ** 1/float(2))\n",
        "\n",
        "        # Apply masking\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 1, value=-1e9)\n",
        "\n",
        "        # Convert attention scores into probability distributions\n",
        "        softmax_attn_scores = softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # Compute Weighted Values\n",
        "        output = matmul(softmax_attn_scores, v)\n",
        "\n",
        "        # Reshape the weighted values\n",
        "        # Transpose: bs x seq-length x num-heads x heads_dim -> bs x seq-length x num-heads x heads_dim)\n",
        "        output = output.transpose(1, 2).contiguous().view(bs, seq, self.heads * self.heads_dim)\n",
        "        output_final = self.unify_heads(output)\n",
        "        return output_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ff_size=2096, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiheadedAttention(d_model=d_model, heads=num_heads)\n",
        "        self.attn_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_size, d_model)\n",
        "        )\n",
        "\n",
        "        self.final_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.do = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, embeddings, mask=None):\n",
        "        # Attn with Pre-Normalization\n",
        "        embeddings = embeddings + self.do(self.self_attn(self.attn_norm(embeddings), mask=mask))\n",
        "        # FeedForward with Pre-Normalization\n",
        "        embeddings = embeddings + self.do(self.ff(self.final_norm(embeddings)))\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, d_model, num_heads=2, num_layers=2):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.enc_layers = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            self.enc_layers.append(TransformerEncoderLayer(d_model, num_heads=num_heads))\n",
        "\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, embeddings, mask=None):\n",
        "\n",
        "        hidden_states = []\n",
        "        for layer in self.enc_layers:\n",
        "            embeddings = layer(embeddings, mask)\n",
        "            hidden_states.append(embeddings)\n",
        "\n",
        "        return hidden_states, self.norm(hidden_states[-1])\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     dim_model = 512\n",
        "#     src_embeddings = torch.rand(2, 32, dim_model)\n",
        "#     model_encoder = TransformerEncoder(d_model=dim_model, num_heads=8, num_layers=6)\n",
        "#     hidden, output = model_encoder(src_embeddings)\n",
        "#     print(len(hidden))\n",
        "#     print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ff_size=2096, dropout=0.1):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiheadedAttention(d_model=d_model, heads=num_heads)\n",
        "        self.attn_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.encoder_decoder_attn = MultiheadedAttention(d_model=d_model, heads=num_heads)\n",
        "\n",
        "        self.encoder_decoder_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_size, d_model)\n",
        "        )\n",
        "\n",
        "        self.final_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.do = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, embeddings, memory, src_mask=None, trg_mask=None):\n",
        "        # Attn with Pre-Normalization\n",
        "        embeddings = embeddings + self.do(self.self_attn(self.attn_norm(embeddings), mask=trg_mask))\n",
        "\n",
        "        # Cross Attn with Pre-Normalization\n",
        "        embeddings = embeddings + self.do(self.encoder_decoder_attn(self.encoder_decoder_norm(embeddings), kv=memory, mask=src_mask))\n",
        "\n",
        "        # FeedForward with Pre-Normalization\n",
        "        embeddings = embeddings + self.do(self.ff(self.final_norm(embeddings)))\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, d_model, num_heads=2, num_layers=2):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "\n",
        "        self.enc_layers = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            self.enc_layers.append(TransformerDecoderLayer(d_model, num_heads=num_heads))\n",
        "\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, embeddings, memory, src_mask=None, trg_mask=None):\n",
        "\n",
        "        hidden_states = []\n",
        "        for layer in self.enc_layers:\n",
        "            embeddings = layer(embeddings, memory, src_mask, trg_mask)\n",
        "            hidden_states.append(embeddings)\n",
        "\n",
        "        return hidden_states, self.norm(hidden_states[-1])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # dim_model = 512\n",
        "    # src_embeddings = torch.rand(2, 32, dim_model)\n",
        "    src_embeddings = X_test_tensor[:10]\n",
        "    batch_size, sequence_length, dim_model = src_embeddings.shape\n",
        "    model_encoder = TransformerEncoder(d_model=dim_model, num_heads=1, num_layers=2)\n",
        "    model_decoder = TransformerDecoder(d_model=dim_model, num_heads=1, num_layers=2)\n",
        "    hidden, output = model_encoder(src_embeddings)\n",
        "    print(len(hidden))\n",
        "    print(output.shape)\n",
        "\n",
        "    trg_embeddings = torch.rand(batch_size, sequence_length, dim_model)\n",
        "    hidden, output = model_decoder(trg_embeddings, output)\n",
        "    print(len(hidden))\n",
        "    print(output.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Expected target size [19, 6], got [19]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[37], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m     26\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m---> 27\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels\u001b[39m.\u001b[39;49mlong())\n\u001b[0;32m     29\u001b[0m \u001b[39m# Backward and optimize\u001b[39;00m\n\u001b[0;32m     30\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\Sepehr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\Sepehr\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\Sepehr\\lib\\site-packages\\torch\\nn\\functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3024\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3025\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3026\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Expected target size [19, 6], got [19]"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.0001\n",
        "num_epochs = 5\n",
        "\n",
        "\n",
        "num_classes = 2\n",
        "input_size = 6\n",
        "hidden_size = 12\n",
        "num_layers = 2\n",
        "rnn = RNN(input_size, hidden_size, num_layers, num_classes)\n",
        "# model = MultiHeadAttention(6, 1)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "# Train the model\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # origin shape: [N, 1, 28, 28]\n",
        "        # resized: [N, 28, 28]\n",
        "        images = images\n",
        "        labels = labels\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels.long())\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'test_loader' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m n_correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      5\u001b[0m n_samples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m test_loader:\n\u001b[0;32m      7\u001b[0m     \u001b[39m# images = images.to(device)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[39m# labels = labels.to(device)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     outputs \u001b[39m=\u001b[39m model(images)\n\u001b[0;32m     10\u001b[0m     \u001b[39m# max returns (value ,index)\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'test_loader' is not defined"
          ]
        }
      ],
      "source": [
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_loader:\n",
        "        # images = images.to(device)\n",
        "        # labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the {len(test_dataset)} test images: {acc} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1HeadAttention: 94.8051948051948%\n",
            "2HeadAttention: 94.8051948051948%\n",
            "3HeadAttention: 77.92207792207792%\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for j in range(1,6):\n",
        "    max = 0\n",
        "    for i in range(10):\n",
        "        model = MultiHeadAttention(6, j)\n",
        "\n",
        "        # Train the model\n",
        "        n_total_steps = len(train_loader)\n",
        "        for epoch in range(num_epochs):\n",
        "            for i, (images, labels) in enumerate(train_loader):  \n",
        "                # origin shape: [N, 1, 28, 28]\n",
        "                # resized: [N, 28, 28]\n",
        "                images = images\n",
        "                labels = labels\n",
        "                \n",
        "                # Forward pass\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels.long())\n",
        "                \n",
        "                # Backward and optimize\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                # print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Test the model\n",
        "        # In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "        with torch.no_grad():\n",
        "            n_correct = 0\n",
        "            n_samples = 0\n",
        "            for images, labels in test_loader:\n",
        "                # images = images.to(device)\n",
        "                # labels = labels.to(device)\n",
        "                outputs = model(images)\n",
        "                # max returns (value ,index)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                n_samples += labels.size(0)\n",
        "                n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            acc = 100.0 * n_correct / n_samples\n",
        "            # print(f'Accuracy of the network on the {len(test_dataset)} test images: {acc} %')\n",
        "        \n",
        "        if acc > max:\n",
        "            # torch.save(f\"attention{j}.pt\", model)\n",
        "            max = acc\n",
        "    print(f\"{j}HeadAttention: {max}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = MultiHeadAttention(6, 1)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Additional information\n",
        "EPOCH = 5\n",
        "PATH = \"model.pt\"\n",
        "LOSS = 0.4\n",
        "\n",
        "torch.save({\n",
        "            'epoch': EPOCH,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': LOSS,\n",
        "            }, PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MultiHeadAttention(\n",
              "  (to_query): Linear(in_features=6, out_features=6, bias=True)\n",
              "  (to_key): Linear(in_features=6, out_features=6, bias=True)\n",
              "  (to_value): Linear(in_features=6, out_features=6, bias=True)\n",
              "  (unify_heads): Linear(in_features=6, out_features=6, bias=True)\n",
              "  (fc_final): Linear(in_features=6, out_features=2, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              "  (relu): ReLU()\n",
              "  (dropout): Dropout(p=0.4, inplace=False)\n",
              "  (norm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint = torch.load(PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "model.eval()\n",
        "# - or -\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = MultiHeadAttention(6, 1)\n",
        "PATH = \"1HeadAttention.pt\"\n",
        "model.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "151WI55ElrOD",
        "outputId": "26266119-370a-4495-cb7f-c27fd7e78334"
      },
      "outputs": [],
      "source": [
        "# model = MultiHeadAttention(6, 1)\n",
        "\n",
        "example = torch.Tensor(batch_size, max, input_size)\n",
        "\n",
        "traced_script_module = torch.jit.trace(model, example)\n",
        "\n",
        "output = traced_script_module(torch.ones(batch_size, max, input_size))\n",
        "\n",
        "sm = torch.jit.script(model)\n",
        "\n",
        "traced_script_module.save(\"Attention.pt\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zYjxy8RQC_g5"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Sepehr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "d1659cde8a93040d23620c3c1e865362d4c35e4c4043f9f9355f7f461e1df95c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
